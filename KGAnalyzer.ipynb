{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22496a46-8fa0-41c0-a580-bdf55c78c35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autoawq\n",
      "  Downloading autoawq-0.2.6-cp310-cp310-manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting einops\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting txtai[all]\n",
      "  Downloading txtai-7.3.0-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting torch==2.3.1 (from autoawq)\n",
      "  Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting transformers>=4.35.0 (from autoawq)\n",
      "  Downloading transformers-4.43.3-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers>=0.12.1 (from autoawq)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from autoawq)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting accelerate (from autoawq)\n",
      "  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting datasets (from autoawq)\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting zstandard (from autoawq)\n",
      "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting autoawq-kernels (from autoawq)\n",
      "  Downloading autoawq_kernels-0.0.7-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->autoawq) (3.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->autoawq) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->autoawq) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->autoawq) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->autoawq) (2023.4.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.1->autoawq)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.1->autoawq)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.1->autoawq)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.1->autoawq)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.1->autoawq)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.1->autoawq)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.1->autoawq)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.1->autoawq)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.1->autoawq)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.1->autoawq)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.1->autoawq)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.3.1 (from torch==2.3.1->autoawq)\n",
      "  Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1->autoawq)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting faiss-cpu>=1.7.1.post2 (from txtai[all])\n",
      "  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting huggingface-hub>=0.9.0 (from txtai[all])\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.18.4 in /usr/local/lib/python3.10/dist-packages (from txtai[all]) (1.24.1)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.10/dist-packages (from txtai[all]) (6.0.1)\n",
      "Collecting regex>=2022.8.17 (from txtai[all])\n",
      "  Downloading regex-2024.7.24-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp>=3.8.1 (from txtai[all])\n",
      "  Downloading aiohttp-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting fastapi>=0.94.0 (from txtai[all])\n",
      "  Downloading fastapi-0.112.0-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting msgpack>=1.0.7 (from txtai[all])\n",
      "  Downloading msgpack-1.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from txtai[all]) (9.3.0)\n",
      "Collecting python-multipart>=0.0.7 (from txtai[all])\n",
      "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting uvicorn>=0.12.1 (from txtai[all])\n",
      "  Downloading uvicorn-0.30.5-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting apache-libcloud>=3.3.1 (from txtai[all])\n",
      "  Downloading apache_libcloud-3.8.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting rich>=12.0.1 (from txtai[all])\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting duckdb>=0.7.1 (from txtai[all])\n",
      "  Downloading duckdb-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (762 bytes)\n",
      "Collecting sqlalchemy>=2.0.20 (from txtai[all])\n",
      "  Downloading SQLAlchemy-2.0.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting grand-cypher>=0.6.0 (from txtai[all])\n",
      "  Downloading grand-cypher-0.9.0.tar.gz (22 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting grand-graph>=0.5.0 (from txtai[all])\n",
      "  Downloading grand-graph-0.5.2.tar.gz (28 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting python-louvain>=0.16 (from txtai[all])\n",
      "  Downloading python-louvain-0.16.tar.gz (204 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.6/204.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting onnx>=1.11.0 (from txtai[all])\n",
      "  Downloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting onnxruntime>=1.11.0 (from txtai[all])\n",
      "  Downloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting soundfile>=0.10.3.post1 (from txtai[all])\n",
      "  Downloading soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl.metadata (14 kB)\n",
      "Collecting scipy>=1.4.1 (from txtai[all])\n",
      "  Downloading scipy-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ttstokenizer>=1.0.0 (from txtai[all])\n",
      "  Downloading ttstokenizer-1.0.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.9.3 in /usr/local/lib/python3.10/dist-packages (from txtai[all]) (4.12.2)\n",
      "Collecting nltk>=3.5 (from txtai[all])\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting pandas>=1.1.0 (from txtai[all])\n",
      "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting tika>=1.24 (from txtai[all])\n",
      "  Downloading tika-2.6.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting imagehash>=4.2.1 (from txtai[all])\n",
      "  Downloading ImageHash-4.3.1-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting timm>=0.4.12 (from txtai[all])\n",
      "  Downloading timm-1.0.8-py3-none-any.whl.metadata (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting litellm>=1.37.16 (from txtai[all])\n",
      "  Downloading litellm-1.42.12-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting llama-cpp-python>=0.2.75 (from txtai[all])\n",
      "  Downloading llama_cpp_python-0.2.85.tar.gz (49.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting fasttext>=0.9.2 (from txtai[all])\n",
      "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting sentencepiece>=0.1.91 (from txtai[all])\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting bitsandbytes>=0.42.0 (from txtai[all])\n",
      "  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
      "Collecting onnxmltools>=1.9.1 (from txtai[all])\n",
      "  Downloading onnxmltools-1.12.0-py2.py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting peft>=0.8.1 (from txtai[all])\n",
      "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting skl2onnx>=1.9.1 (from txtai[all])\n",
      "  Downloading skl2onnx-1.17.0-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting annoy>=1.16.3 (from txtai[all])\n",
      "  Downloading annoy-1.17.3.tar.gz (647 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.5/647.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting hnswlib>=0.5.0 (from txtai[all])\n",
      "  Downloading hnswlib-0.8.0.tar.gz (36 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pgvector>=0.2.5 (from txtai[all])\n",
      "  Downloading pgvector-0.3.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pymagnitude-lite>=0.1.43 (from txtai[all])\n",
      "  Downloading pymagnitude_lite-0.1.143-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting scikit-learn>=0.23.1 (from txtai[all])\n",
      "  Downloading scikit_learn-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting sentence-transformers>=2.2.0 (from txtai[all])\n",
      "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting croniter>=1.2.0 (from txtai[all])\n",
      "  Downloading croniter-3.0.3-py2.py3-none-any.whl.metadata (28 kB)\n",
      "Collecting openpyxl>=3.0.9 (from txtai[all])\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from txtai[all]) (2.31.0)\n",
      "Collecting xmltodict>=0.12.0 (from txtai[all])\n",
      "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->autoawq) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->autoawq) (5.9.6)\n",
      "Collecting safetensors>=0.3.1 (from accelerate->autoawq)\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp>=3.8.1->txtai[all])\n",
      "  Downloading aiohappyeyeballs-2.3.4-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp>=3.8.1->txtai[all])\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.8.1->txtai[all]) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp>=3.8.1->txtai[all])\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp>=3.8.1->txtai[all])\n",
      "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp>=3.8.1->txtai[all])\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp>=3.8.1->txtai[all])\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.9.3->txtai[all]) (2.5)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from croniter>=1.2.0->txtai[all]) (2.8.2)\n",
      "Collecting pytz>2021.1 (from croniter>=1.2.0->txtai[all])\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.94.0->txtai[all])\n",
      "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 (from fastapi>=0.94.0->txtai[all])\n",
      "  Downloading pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pybind11>=2.2 (from fasttext>=0.9.2->txtai[all])\n",
      "  Using cached pybind11-2.13.1-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext>=0.9.2->txtai[all]) (68.2.2)\n",
      "Collecting grandiso (from grand-cypher>=0.6.0->txtai[all])\n",
      "  Downloading grandiso-2.2.0.tar.gz (16 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting lark-parser (from grand-cypher>=0.6.0->txtai[all])\n",
      "  Downloading lark_parser-0.12.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting cachetools (from grand-graph>=0.5.0->txtai[all])\n",
      "  Downloading cachetools-5.4.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting fsspec (from torch==2.3.1->autoawq)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting tqdm>=4.42.1 (from huggingface-hub>=0.9.0->txtai[all])\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting PyWavelets (from imagehash>=4.2.1->txtai[all])\n",
      "  Downloading pywavelets-1.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting click (from litellm>=1.37.16->txtai[all])\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting importlib-metadata>=6.8.0 (from litellm>=1.37.16->txtai[all])\n",
      "  Downloading importlib_metadata-8.2.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting jsonschema<5.0.0,>=4.22.0 (from litellm>=1.37.16->txtai[all])\n",
      "  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting openai>=1.27.0 (from litellm>=1.37.16->txtai[all])\n",
      "  Downloading openai-1.38.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting python-dotenv>=0.2.0 (from litellm>=1.37.16->txtai[all])\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting tiktoken>=0.7.0 (from litellm>=1.37.16->txtai[all])\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python>=0.2.75->txtai[all])\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting joblib (from nltk>=3.5->txtai[all])\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting protobuf>=3.20.2 (from onnx>=1.11.0->txtai[all])\n",
      "  Downloading protobuf-5.27.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting coloredlogs (from onnxruntime>=1.11.0->txtai[all])\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.11.0->txtai[all])\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting et-xmlfile (from openpyxl>=3.0.9->txtai[all])\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.1.0->txtai[all])\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting fasteners>=0.14.1 (from pymagnitude-lite>=0.1.43->txtai[all])\n",
      "  Downloading fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting lz4>=1.0.0 (from pymagnitude-lite>=0.1.43->txtai[all])\n",
      "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting xxhash>=1.0.1 (from pymagnitude-lite>=0.1.43->txtai[all])\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->txtai[all]) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->txtai[all]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->txtai[all]) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->txtai[all]) (2022.12.7)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=12.0.1->txtai[all])\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.1->txtai[all]) (2.16.1)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=0.23.1->txtai[all])\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting onnxconverter-common>=1.7.0 (from skl2onnx>=1.9.1->txtai[all])\n",
      "  Downloading onnxconverter_common-1.14.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.10.3.post1->txtai[all]) (1.16.0)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy>=2.0.20->txtai[all])\n",
      "  Downloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm>=0.4.12->txtai[all]) (0.16.0+cu118)\n",
      "Collecting anyascii>=0.3.1 (from ttstokenizer>=1.0.0->txtai[all])\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting inflect>=0.3.1 (from ttstokenizer>=1.0.0->txtai[all])\n",
      "  Downloading inflect-7.3.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.8 (from uvicorn>=0.12.1->txtai[all])\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets->autoawq)\n",
      "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting pyarrow-hotfix (from datasets->autoawq)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->autoawq)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting requests>=2.26.0 (from txtai[all])\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting multiprocess (from datasets->autoawq)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec (from torch==2.3.1->autoawq)\n",
      "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.10.3.post1->txtai[all]) (2.21)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib-metadata>=6.8.0->litellm>=1.37.16->txtai[all]) (1.0.0)\n",
      "Requirement already satisfied: more-itertools>=8.5.0 in /usr/lib/python3/dist-packages (from inflect>=0.3.1->ttstokenizer>=1.0.0->txtai[all]) (8.10.0)\n",
      "Collecting typeguard>=4.0.1 (from inflect>=0.3.1->ttstokenizer>=1.0.0->txtai[all])\n",
      "  Downloading typeguard-4.3.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.1->autoawq) (2.1.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.37.16->txtai[all]) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.37.16->txtai[all]) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.37.16->txtai[all]) (0.12.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=12.0.1->txtai[all])\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting protobuf>=3.20.2 (from onnx>=1.11.0->txtai[all])\n",
      "  Downloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.27.0->litellm>=1.37.16->txtai[all]) (4.0.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.27.0->litellm>=1.37.16->txtai[all]) (1.7.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai>=1.27.0->litellm>=1.37.16->txtai[all])\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.27.0->litellm>=1.37.16->txtai[all]) (1.3.0)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.94.0->txtai[all])\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.20.1 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.94.0->txtai[all])\n",
      "  Downloading pydantic_core-2.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil->croniter>=1.2.0->txtai[all]) (1.16.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.11.0->txtai[all])\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.1->autoawq) (1.3.0)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision (from timm>=0.4.12->txtai[all])\n",
      "  Downloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
      "  Downloading torchvision-0.18.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.27.0->litellm>=1.37.16->txtai[all]) (1.1.3)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.27.0->litellm>=1.37.16->txtai[all])\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Downloading autoawq-0.2.6-cp310-cp310-manylinux2014_x86_64.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading apache_libcloud-3.8.0-py2.py3-none-any.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading croniter-3.0.3-py2.py3-none-any.whl (22 kB)\n",
      "Downloading duckdb-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.5/18.5 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.112.0-py3-none-any.whl (93 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.5/417.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.5/296.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading litellm-1.42.12-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading msgpack-1.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (385 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.1/385.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading onnx-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading onnxmltools-1.12.0-py2.py3-none-any.whl (329 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.0/329.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.9/250.9 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pgvector-0.3.2-py2.py3-none-any.whl (24 kB)\n",
      "Downloading pymagnitude_lite-0.1.143-py3-none-any.whl (34 kB)\n",
      "Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Downloading regex-2024.7.24-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (776 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.5/776.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.1/41.1 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading skl2onnx-1.17.0-py2.py3-none-any.whl (298 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.4/298.4 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading SQLAlchemy-2.0.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading timm-1.0.8-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.43.3-py3-none-any.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ttstokenizer-1.0.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading uvicorn-0.30.5-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading autoawq_kernels-0.0.7-cp310-cp310-manylinux2014_x86_64.whl (33.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.6/33.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading txtai-7.3.0-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.7/227.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.3.4-py3-none-any.whl (12 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (616 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.0/616.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_metadata-8.2.0-py3-none-any.whl (25 kB)\n",
      "Downloading inflect-7.3.1-py3-none-any.whl (34 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.5/88.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnxconverter_common-1.14.0-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-3.20.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.38.0-py3-none-any.whl (335 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.9/335.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached pybind11-2.13.1-py3-none-any.whl (238 kB)\n",
      "Downloading pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.9/423.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-5.4.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading lark_parser-0.12.0-py2.py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading pywavelets-1.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.18.1-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading typeguard-4.3.0-py3-none-any.whl (35 kB)\n",
      "Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: annoy, fasttext, grand-cypher, grand-graph, hnswlib, llama-cpp-python, python-louvain, tika, grandiso\n",
      "  Building wheel for annoy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for annoy: filename=annoy-1.17.3-cp310-cp310-linux_x86_64.whl size=550722 sha256=dc63cfb57ccfbf664a36bb3bbcf43f7712a6d38a35686801d9f807a482e80bae\n",
      "  Stored in directory: /root/.cache/pip/wheels/64/8a/da/f714bcf46c5efdcfcac0559e63370c21abe961c48e3992465a\n",
      "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.3-cp310-cp310-linux_x86_64.whl size=4246765 sha256=1b6070d9b7b0c221e7fa3392eef3fbcc30b8f42818a2a09779c01bcd929351ae\n",
      "  Stored in directory: /root/.cache/pip/wheels/0d/a2/00/81db54d3e6a8199b829d58e02cec2ddb20ce3e59fad8d3c92a\n",
      "  Building wheel for grand-cypher (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for grand-cypher: filename=grand_cypher-0.9.0-py3-none-any.whl size=22995 sha256=d0c8a83f39b1416bc78556fb09f32c2d964fbe554e6740bb649578403bdc5cf4\n",
      "  Stored in directory: /root/.cache/pip/wheels/cb/9e/96/5eeea9b212d98b0d7364a5cc19a86a41c7365926d853ca4616\n",
      "  Building wheel for grand-graph (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for grand-graph: filename=grand_graph-0.5.2-py3-none-any.whl size=36278 sha256=abe4962d6852a77c7cdcf6ace584acf7a739bbdcc0ecf5ecdfce36604da325d4\n",
      "  Stored in directory: /root/.cache/pip/wheels/55/a9/9f/235eca0ae1fbcc45c7fa926176caaeec9cfc203c23f656ca9c\n",
      "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hnswlib: filename=hnswlib-0.8.0-cp310-cp310-linux_x86_64.whl size=2331039 sha256=08777ea621541c672397a64a8e59eeac4877490ffb37a9437a90765f2e54d9f3\n",
      "  Stored in directory: /root/.cache/pip/wheels/af/a9/3e/3e5d59ee41664eb31a4e6de67d1846f86d16d93c45f277c4e7\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.85-cp310-cp310-linux_x86_64.whl size=2873154 sha256=ca1d02b8b26b4e5e0e9dbfe088ffc2b55d48a0a6b204edc24c1754e0184d5778\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e8/4e/29a754f9175ef52b6481cd75e3af4de38bf6dfa9c2972f75d4\n",
      "  Building wheel for python-louvain (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-louvain: filename=python_louvain-0.16-py3-none-any.whl size=9389 sha256=3488e40c0c51bc0c77886027ebb7d61ae35b0eca9e4eb0304f4980c7e226725f\n",
      "  Stored in directory: /root/.cache/pip/wheels/d0/b0/d7/6dd26c3817810fa379088eaeb755a01d9a2a411c37632079d1\n",
      "  Building wheel for tika (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tika: filename=tika-2.6.0-py3-none-any.whl size=32621 sha256=54ec7bf5be895053891d5d30762667890344ecf03231bd9b98b1a7f1d9788693\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/71/c7/b757709531121b1700cffda5b6b0d4aad095fb507ec84316d0\n",
      "  Building wheel for grandiso (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for grandiso: filename=grandiso-2.2.0-py3-none-any.whl size=15811 sha256=cf37f2d692d4d3008ce1cc867b30d1171f1f5beb4f9d17bd975348a0aa7f28e5\n",
      "  Stored in directory: /root/.cache/pip/wheels/61/f2/5a/9c49f5896a0ba3f2419a34d7ad72e67256e9a336b4eb7f9c0c\n",
      "Successfully built annoy fasttext grand-cypher grand-graph hnswlib llama-cpp-python python-louvain tika grandiso\n",
      "Installing collected packages: sentencepiece, pytz, lark-parser, flatbuffers, annoy, zstandard, xxhash, xmltodict, tzdata, typing-extensions, triton, tqdm, threadpoolctl, scipy, safetensors, requests, regex, PyWavelets, python-multipart, python-louvain, python-dotenv, pybind11, pyarrow-hotfix, pyarrow, protobuf, pgvector, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, multidict, msgpack, mdurl, lz4, joblib, importlib-metadata, humanfriendly, hnswlib, h11, greenlet, grandiso, fsspec, frozenlist, fasteners, faiss-cpu, et-xmlfile, einops, duckdb, diskcache, dill, click, cachetools, async-timeout, anyascii, annotated-types, aiohappyeyeballs, yarl, uvicorn, typeguard, tiktoken, tika, starlette, sqlalchemy, soundfile, scikit-learn, pymagnitude-lite, pydantic-core, pandas, openpyxl, onnx, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nltk, multiprocess, markdown-it-py, llama-cpp-python, imagehash, huggingface-hub, httpcore, grand-cypher, fasttext, croniter, coloredlogs, apache-libcloud, aiosignal, tokenizers, rich, pydantic, onnxruntime, onnxmltools, onnxconverter-common, nvidia-cusolver-cu12, jsonschema, inflect, httpx, grand-graph, aiohttp, ttstokenizer, transformers, torch, skl2onnx, openai, fastapi, txtai, torchvision, sentence-transformers, litellm, datasets, bitsandbytes, autoawq-kernels, accelerate, timm, peft, autoawq\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.6.4\n",
      "    Uninstalling importlib-metadata-4.6.4:\n",
      "      Successfully uninstalled importlib-metadata-4.6.4\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 4.19.2\n",
      "    Uninstalling jsonschema-4.19.2:\n",
      "      Successfully uninstalled jsonschema-4.19.2\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.0+cu118\n",
      "    Uninstalling torch-2.1.0+cu118:\n",
      "      Successfully uninstalled torch-2.1.0+cu118\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.16.0+cu118\n",
      "    Uninstalling torchvision-0.16.0+cu118:\n",
      "      Successfully uninstalled torchvision-0.16.0+cu118\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyWavelets-1.6.0 accelerate-0.33.0 aiohappyeyeballs-2.3.4 aiohttp-3.10.0 aiosignal-1.3.1 annotated-types-0.7.0 annoy-1.17.3 anyascii-0.3.2 apache-libcloud-3.8.0 async-timeout-4.0.3 autoawq-0.2.6 autoawq-kernels-0.0.7 bitsandbytes-0.43.3 cachetools-5.4.0 click-8.1.7 coloredlogs-15.0.1 croniter-3.0.3 datasets-2.20.0 dill-0.3.8 diskcache-5.6.3 duckdb-1.0.0 einops-0.8.0 et-xmlfile-1.1.0 faiss-cpu-1.8.0.post1 fastapi-0.112.0 fasteners-0.19 fasttext-0.9.3 flatbuffers-24.3.25 frozenlist-1.4.1 fsspec-2024.5.0 grand-cypher-0.9.0 grand-graph-0.5.2 grandiso-2.2.0 greenlet-3.0.3 h11-0.14.0 hnswlib-0.8.0 httpcore-1.0.5 httpx-0.27.0 huggingface-hub-0.24.5 humanfriendly-10.0 imagehash-4.3.1 importlib-metadata-8.2.0 inflect-7.3.1 joblib-1.4.2 jsonschema-4.23.0 lark-parser-0.12.0 litellm-1.42.12 llama-cpp-python-0.2.85 lz4-4.3.3 markdown-it-py-3.0.0 mdurl-0.1.2 msgpack-1.0.8 multidict-6.0.5 multiprocess-0.70.16 nltk-3.8.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 onnx-1.16.2 onnxconverter-common-1.14.0 onnxmltools-1.12.0 onnxruntime-1.18.1 openai-1.38.0 openpyxl-3.1.5 pandas-2.2.2 peft-0.12.0 pgvector-0.3.2 protobuf-3.20.2 pyarrow-17.0.0 pyarrow-hotfix-0.6 pybind11-2.13.1 pydantic-2.8.2 pydantic-core-2.20.1 pymagnitude-lite-0.1.143 python-dotenv-1.0.1 python-louvain-0.16 python-multipart-0.0.9 pytz-2024.1 regex-2024.7.24 requests-2.32.3 rich-13.7.1 safetensors-0.4.3 scikit-learn-1.5.1 scipy-1.14.0 sentence-transformers-3.0.1 sentencepiece-0.2.0 skl2onnx-1.17.0 soundfile-0.12.1 sqlalchemy-2.0.31 starlette-0.37.2 threadpoolctl-3.5.0 tika-2.6.0 tiktoken-0.7.0 timm-1.0.8 tokenizers-0.19.1 torch-2.3.1 torchvision-0.18.1 tqdm-4.66.5 transformers-4.43.3 triton-2.3.1 ttstokenizer-1.0.0 txtai-7.3.0 typeguard-4.3.0 typing-extensions-4.12.2 tzdata-2024.1 uvicorn-0.30.5 xmltodict-0.13.0 xxhash-3.4.1 yarl-1.9.4 zstandard-0.23.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install autoawq txtai[all] einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b597ef3-3431-4a58-afb0-b967e8c6447c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.53.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (305 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.2/305.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.53.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.2.1 cycler-0.12.1 fonttools-4.53.1 kiwisolver-1.4.5 matplotlib-3.9.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d318b7fc-632b-4879-8fd6-b4f52b486e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import networkx as nx\n",
    "import random\n",
    "from txtai import LLM, Embeddings\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "class KnowledgeGraphAnalyzer:\n",
    "    \"\"\"\n",
    "    A class used to analyze knowledge graphs.\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    embeddings_path : str\n",
    "        The path to the embeddings file.\n",
    "    llm_model : str\n",
    "        The name of the LLM model to use.\n",
    "    csv_filename : str\n",
    "        The filename of the CSV file to use.\n",
    "    embeddings_config : dict\n",
    "        The configuration for the embeddings.\n",
    "    embeddings : Embeddings\n",
    "        The embeddings object.\n",
    "    llm : LLM\n",
    "        The LLM object.\n",
    "    nx_graph : nx.Graph\n",
    "        The NetworkX graph object.\n",
    "    id_mapping : dict\n",
    "        A dictionary mapping node IDs to node objects.\n",
    "\n",
    "    Methods:\n",
    "    -------\n",
    "    process_text_file(file_path)\n",
    "        Process a text file and generate topics.\n",
    "    generate_topics(paragraphs)\n",
    "        Generate topics for a list of paragraphs.\n",
    "    analyze_graph()\n",
    "        Analyze the knowledge graph.\n",
    "    save_embeddings(filename)\n",
    "        Save the embeddings to a file.\n",
    "    load_embeddings(filename)\n",
    "        Load the embeddings from a file.\n",
    "    graph_qa(question)\n",
    "        Perform Q&A on the graph.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embeddings_path=\"intfloat/e5-large\", llm_model=\"TheBloke/Mistral-7B-OpenOrca-AWQ\", \n",
    "                 embeddings_config=None, csv_filename=\"hadenpa_lore_expanded.csv\"):\n",
    "        \"\"\"\n",
    "        Initialize the KnowledgeGraphAnalyzer object.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        embeddings_path : str\n",
    "            The path to the embeddings file.\n",
    "        llm_model : str\n",
    "            The name of the LLM model to use.\n",
    "        csv_filename : str\n",
    "            The filename of the CSV file to use.\n",
    "        embeddings_config : dict\n",
    "            The configuration for the embeddings.\n",
    "        \"\"\"\n",
    "        self.embeddings_path = embeddings_path\n",
    "        self.llm_model = llm_model\n",
    "        self.csv_filename = csv_filename\n",
    "        \n",
    "        if embeddings_config is None:\n",
    "            embeddings_config = {\n",
    "                \"autoid\": \"uuid5\",\n",
    "                \"instructions\": {\"query\": \"query: \", \"data\": \"passage: \"},\n",
    "                \"content\": True,\n",
    "                \"graph\": {\"approximate\": False, \"minscore\": 0.7}\n",
    "            }\n",
    "        \n",
    "        self.embeddings = Embeddings(path=self.embeddings_path, **embeddings_config)\n",
    "        self.llm = LLM(self.llm_model)\n",
    "        self.llm.generator.llm.pipeline.tokenizer.pad_token_id = self.llm.generator.llm.pipeline.tokenizer.eos_token_id\n",
    "        self.nx_graph = None\n",
    "        self.id_mapping = None\n",
    "\n",
    "    def process_text_file(self, file_path: str, max_paragraph_length: int = 1300, overlap: int = 200) -> list:\n",
    "        \"\"\"\n",
    "        Process a text file and generate topics.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        file_path : str\n",
    "            The path to the text file.\n",
    "        max_paragraph_length : int\n",
    "            The maximum length of a paragraph.\n",
    "        overlap : int\n",
    "            The overlap between paragraphs.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        list\n",
    "            A list of paragraphs.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        \n",
    "        text = re.sub(r'\\n+', ' ', text)\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        \n",
    "        paragraphs = []\n",
    "        current_paragraph = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if len(current_paragraph) + len(sentence) + 1 <= max_paragraph_length:\n",
    "                current_paragraph += (\" \" + sentence if current_paragraph else sentence)\n",
    "            else:\n",
    "                paragraphs.append(current_paragraph.strip())\n",
    "                overlap_text = current_paragraph[-overlap:] if len(current_paragraph) > overlap else current_paragraph\n",
    "                current_paragraph = overlap_text + \" \" + sentence\n",
    "        \n",
    "        if current_paragraph:\n",
    "            paragraphs.append(current_paragraph.strip())\n",
    "        \n",
    "        final_paragraphs = []\n",
    "        for paragraph in paragraphs:\n",
    "            while len(paragraph) > max_paragraph_length:\n",
    "                split_point = max_paragraph_length - overlap\n",
    "                final_paragraphs.append(paragraph[:split_point].strip())\n",
    "                paragraph = paragraph[split_point - overlap:].strip()\n",
    "            final_paragraphs.append(paragraph.strip())\n",
    "        \n",
    "        return final_paragraphs\n",
    "\n",
    "    def generate_topics(self, paragraphs: list, batch_size: int = 5) -> None:\n",
    "        \"\"\"\n",
    "        Generate topics for a list of paragraphs.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        paragraphs : list\n",
    "            A list of paragraphs.\n",
    "        batch_size : int\n",
    "            The batch size for generating topics.\n",
    "        \"\"\"\n",
    "        existing_paragraphs = set()\n",
    "        if os.path.exists(self.csv_filename):\n",
    "            df = pd.read_csv(self.csv_filename)\n",
    "            existing_paragraphs = set(df['Paragraph'].tolist())\n",
    "\n",
    "        new_paragraphs = [p for p in paragraphs if p not in existing_paragraphs]\n",
    "        if not new_paragraphs:\n",
    "            print(\"No new paragraphs to process.\")\n",
    "            return\n",
    "\n",
    "        self.embeddings.upsert([(str(i), text, None) for i, text in enumerate(new_paragraphs)])\n",
    "\n",
    "        batch = []\n",
    "        for uid in tqdm(range(len(new_paragraphs)), desc=\"Inferring topics\"):\n",
    "            text = new_paragraphs[uid]\n",
    "            batch.append((uid, text))\n",
    "            if len(batch) == batch_size:\n",
    "                self._process_topic_batch(batch)\n",
    "                batch = []\n",
    "\n",
    "        if batch:\n",
    "            self._process_topic_batch(batch)\n",
    "\n",
    "    def _process_topic_batch(self, batch: list) -> None:\n",
    "        \"\"\"\n",
    "        Process a batch of paragraphs and generate topics.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        batch : list\n",
    "            A list of paragraphs.\n",
    "        \"\"\"\n",
    "        prompt_template = \"Create a simple, concise topic for the following text. Only return the topic name.\\n\\nText: {text}\"\n",
    "        prompts = [[{\"role\": \"user\", \"content\": prompt_template.format(text=text)}] for _, text in batch]\n",
    "\n",
    "        for uid, topic in zip(\n",
    "            [uid for uid, _ in batch],\n",
    "            self.llm(prompts, maxlength=2048, batch_size=len(batch))\n",
    "        ):\n",
    "            self.embeddings.graph.addattribute(uid, \"topic\", topic)\n",
    "            topics = self.embeddings.graph.topics\n",
    "            if topics is not None:\n",
    "                if topic not in topics:\n",
    "                    topics[topic] = []\n",
    "                topics[topic].append(uid)\n",
    "\n",
    "    def analyze_graph(self) -> None:\n",
    "        \"\"\"\n",
    "        Analyze the knowledge graph.\n",
    "        \"\"\"\n",
    "        self.nx_graph = nx.Graph(self.embeddings.graph.backend)\n",
    "        \n",
    "        print(f\"Number of nodes: {self.nx_graph.number_of_nodes()}\")\n",
    "        print(f\"Number of edges: {self.nx_graph.number_of_edges()}\")\n",
    "        \n",
    "        print(\"\\nBasic Graph Properties:\")\n",
    "        self._print_basic_properties()\n",
    "        \n",
    "        print(\"\\nDegree Distribution:\")\n",
    "        degree_dist = self._get_degree_distribution()\n",
    "        self._print_degree_distribution(degree_dist)\n",
    "        \n",
    "        print(\"\\nCentrality Analysis:\")\n",
    "        self._centrality_analysis()\n",
    "        \n",
    "        print(\"\\nClustering Analysis:\")\n",
    "        self._clustering_analysis()\n",
    "        \n",
    "        print(\"\\nConnected Components:\")\n",
    "        self._component_analysis()\n",
    "        \n",
    "        print(\"\\nTopic Distribution:\")\n",
    "        self._topic_distribution()\n",
    "\n",
    "    def _print_basic_properties(self) -> None:\n",
    "        \"\"\"\n",
    "        Print basic graph properties.\n",
    "        \"\"\"\n",
    "        print(f\"Is connected: {nx.is_connected(self.nx_graph)}\")\n",
    "        print(f\"Diameter: {nx.diameter(self.nx_graph) if nx.is_connected(self.nx_graph) else 'N/A (Graph is not connected)'}\")\n",
    "        print(f\"Average shortest path length: {nx.average_shortest_path_length(self.nx_graph) if nx.is_connected(self.nx_graph) else 'N/A (Graph is not connected)'}\")\n",
    "        print(f\"Average clustering coefficient: {nx.average_clustering(self.nx_graph)}\")\n",
    "\n",
    "    def _get_degree_distribution(self) -> Counter:\n",
    "        \"\"\"\n",
    "        Get the degree distribution of the graph.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        Counter\n",
    "            A Counter object representing the degree distribution.\n",
    "        \"\"\"\n",
    "        return Counter(dict(self.nx_graph.degree()).values())\n",
    "\n",
    "    def _print_degree_distribution(self, degree_dist: Counter) -> None:\n",
    "        \"\"\"\n",
    "        Print the degree distribution.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        degree_dist : Counter\n",
    "            A Counter object representing the degree distribution.\n",
    "        \"\"\"\n",
    "        for degree, count in sorted(degree_dist.items()):\n",
    "            print(f\"Degree {degree}: {count} nodes\")\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(degree_dist.keys(), degree_dist.values())\n",
    "        plt.xlabel('Degree')\n",
    "        plt.ylabel('Number of Nodes')\n",
    "        plt.title('Degree Distribution')\n",
    "        plt.show()\n",
    "\n",
    "    def _centrality_analysis(self, top_n: int = 10) -> None:\n",
    "        \"\"\"\n",
    "        Perform centrality analysis.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        top_n : int\n",
    "            The number of top nodes to display.\n",
    "        \"\"\"\n",
    "        centrality_measures = {\n",
    "            \"degree\": nx.degree_centrality,\n",
    "            \"betweenness\": nx.betweenness_centrality,\n",
    "            \"closeness\": nx.closeness_centrality\n",
    "        }\n",
    "\n",
    "        for measure, func in centrality_measures.items():\n",
    "            print(f\"\\nTop {top_n} nodes by {measure} centrality:\")\n",
    "            centrality = func(self.nx_graph)\n",
    "            for node, value in sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:top_n]:\n",
    "                print(f\"Node {node}: {value:.4f}\")\n",
    "\n",
    "    def _clustering_analysis(self) -> None:\n",
    "        \"\"\"\n",
    "        Perform clustering analysis.\n",
    "        \"\"\"\n",
    "        clustering_coeffs = nx.clustering(self.nx_graph)\n",
    "        avg_clustering = sum(clustering_coeffs.values()) / len(clustering_coeffs)\n",
    "        print(f\"Average clustering coefficient: {avg_clustering:.4f}\")\n",
    "        \n",
    "        print(\"\\nTop nodes by local clustering coefficient:\")\n",
    "        for node, coeff in sorted(clustering_coeffs.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "            print(f\"Node {node}: {coeff:.4f}\")\n",
    "\n",
    "    def _component_analysis(self) -> None:\n",
    "        \"\"\"\n",
    "        Perform connected component analysis.\n",
    "        \"\"\"\n",
    "        components = list(nx.connected_components(self.nx_graph))\n",
    "        print(f\"Number of connected components: {len(components)}\")\n",
    "        print(f\"Size of the largest component: {len(max(components, key=len))}\")\n",
    "        \n",
    "        component_sizes = [len(c) for c in components]\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(component_sizes, bins=20)\n",
    "        plt.xlabel('Component Size')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Distribution of Connected Component Sizes')\n",
    "        plt.show()\n",
    "\n",
    "    def _topic_distribution(self, top_n: int = 10, plot_n: int = 20) -> None:\n",
    "        \"\"\"\n",
    "        Perform topic distribution analysis.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        top_n : int\n",
    "            The number of top topics to display.\n",
    "        plot_n : int\n",
    "            The number of topics to plot.\n",
    "        \"\"\"\n",
    "        topic_counts = Counter()\n",
    "        for node in self.embeddings.graph.scan():\n",
    "            topic = self.embeddings.graph.attribute(node, \"topic\")\n",
    "            if topic:\n",
    "                topic_counts[topic] += 1\n",
    "        \n",
    "        print(f\"Top {top_n} most common topics:\")\n",
    "        for topic, count in topic_counts.most_common(top_n):\n",
    "            print(f\"{topic}: {count}\")\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        topics, counts = zip(*topic_counts.most_common(plot_n))\n",
    "        plt.bar(topics, counts)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.xlabel('Topics')\n",
    "        plt.ylabel('Number of Nodes')\n",
    "        plt.title(f'Distribution of Top {len(topics)} Topics')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def save_embeddings(self, filename: str = 'hadenpa.tar.gz') -> None:\n",
    "        \"\"\"\n",
    "        Save the embeddings to a file.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        filename : str\n",
    "            The filename to save the embeddings to.\n",
    "        \"\"\"\n",
    "        self.embeddings.save(filename)\n",
    "\n",
    "    def load_embeddings(self, filename: str = 'hadenpa.tar.gz') -> None:\n",
    "        \"\"\"\n",
    "        Load the embeddings from a file.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        filename : str\n",
    "            The filename to load the embeddings from.\n",
    "        \"\"\"\n",
    "        if os.path.exists(filename):\n",
    "            self.embeddings.load(filename)\n",
    "            print(f\"Embeddings loaded from {filename}\")\n",
    "        else:\n",
    "            print(f\"File {filename} not found. Starting with fresh embeddings.\")\n",
    "\n",
    "    def graph_qa(self, question: str, max_path_length: int = 3, num_paths: int = 3) -> str:\n",
    "        \"\"\"\n",
    "        Perform Q&A on the graph.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        question : str\n",
    "            The question to answer.\n",
    "        max_path_length : int\n",
    "            The maximum length of a path.\n",
    "        num_paths : int\n",
    "            The number of paths to consider.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        str\n",
    "            The answer to the question.\n",
    "        \"\"\"\n",
    "        if not self.nx_graph or not self.id_mapping:\n",
    "            self.initialize_graph()\n",
    "\n",
    "        start_nodes = self.embeddings.search(question, 3)\n",
    "        paths = []\n",
    "        for start_node in start_nodes:\n",
    "            if start_node['id'] in self.id_mapping:\n",
    "                nx_node = self.id_mapping[start_node['id']]\n",
    "                paths.extend(self.random_walks(nx_node, max_path_length, num_paths))\n",
    "        path_info = self.extract_path_info(paths)\n",
    "        return self.generate_answer(question, path_info)\n",
    "\n",
    "    def random_walks(self, start_node: str, max_length: int, num_paths: int) -> list:\n",
    "        \"\"\"\n",
    "        Perform random walks on the graph.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        start_node : str\n",
    "            The starting node.\n",
    "        max_length : int\n",
    "            The maximum length of a path.\n",
    "        num_paths : int\n",
    "            The number of paths to consider.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        list\n",
    "            A list of paths.\n",
    "        \"\"\"\n",
    "        paths = []\n",
    "        visited_global = set()\n",
    "        for _ in range(num_paths):\n",
    "            path = [start_node]\n",
    "            visited_local = set([start_node])\n",
    "            for _ in range(max_length - 1):\n",
    "                neighbors = [n for n in self.nx_graph.neighbors(path[-1]) \n",
    "                             if n not in visited_local and n not in visited_global]\n",
    "                if not neighbors:\n",
    "                    break\n",
    "                next_node = random.choice(neighbors)\n",
    "                path.append(next_node)\n",
    "                visited_local.add(next_node)\n",
    "            visited_global.update(visited_local)\n",
    "            paths.append(path)\n",
    "        return paths\n",
    "\n",
    "    def extract_path_info(self, paths: list) -> list:\n",
    "        \"\"\"\n",
    "        Extract information from paths.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        paths : list\n",
    "            A list of paths.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        list\n",
    "            A list of path information.\n",
    "        \"\"\"\n",
    "        seen = set()\n",
    "        path_info = []\n",
    "        for path in paths:\n",
    "            info = []\n",
    "            for node in path:\n",
    "                if node not in seen:\n",
    "                    text = self.embeddings.graph.attribute(node, \"text\")\n",
    "                    topic = self.embeddings.graph.attribute(node, \"topic\")\n",
    "                    info.append({\"node\": node, \"text\": text, \"topic\": topic})\n",
    "                    seen.add(node)\n",
    "            if info:\n",
    "                path_info.append(info)\n",
    "        return path_info\n",
    "\n",
    "    def generate_answer(self, question: str, path_info: list) -> str:\n",
    "        \"\"\"\n",
    "        Generate an answer to a question.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        question : str\n",
    "            The question to answer.\n",
    "        path_info : list\n",
    "            A list of path information.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        str\n",
    "            The answer to the question.\n",
    "        \"\"\"\n",
    "        graph_context = self.format_context(path_info)\n",
    "        vector_results = self.get_diverse_vector_results(question)\n",
    "        vector_context = self.format_vector_results(vector_results)\n",
    "        combined_context = f\"{graph_context}\\n\\nAdditional relevant information:\\n{vector_context}\"\n",
    "        \n",
    "        prompt = f\"\"\"Given the following context extracted from a document graph and vector search, answer the question. \n",
    "Use only the information provided in the context. If the answer cannot be fully determined from the context, \n",
    "say so and provide the best partial answer possible.\n",
    "\n",
    "Context:\n",
    "{combined_context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "        \n",
    "        with open('prompt.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(prompt)\n",
    "        print(f\"Prompt saved to {os.path.abspath('prompt.txt')}\")\n",
    "        \n",
    "        return self.llm(prompt, max_length=8192)\n",
    "\n",
    "    def get_diverse_vector_results(self, question: str, num_results: int = 5, diversity_threshold: float = 0.7) -> list:\n",
    "        \"\"\"\n",
    "        Get diverse vector results.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        question : str\n",
    "            The question to answer.\n",
    "        num_results : int\n",
    "            The number of results to consider.\n",
    "        diversity_threshold : float\n",
    "            The diversity threshold.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        list\n",
    "            A list of diverse vector results.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            all_results = self.embeddings.search(question, num_results * 2)\n",
    "            diverse_results = OrderedDict()\n",
    "            for result in all_results:\n",
    "                if len(diverse_results) >= num_results:\n",
    "                    break\n",
    "                is_diverse = True\n",
    "                for dr in diverse_results.values():\n",
    "                    try:\n",
    "                        similarity = self.embeddings.similarity(result['text'], dr['text'])\n",
    "                        if isinstance(similarity, list):\n",
    "                            similarity = similarity[0][1]\n",
    "                        if not isinstance(similarity, (int, float)):\n",
    "                            raise TypeError(f\"Unexpected similarity type: {type(similarity)}\")\n",
    "                        if similarity >= diversity_threshold:\n",
    "                            is_diverse = False\n",
    "                            break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error calculating similarity: {e}\")\n",
    "                        is_diverse = False\n",
    "                        break\n",
    "                if is_diverse:\n",
    "                    diverse_results[result['id']] = result\n",
    "            return list(diverse_results.values())\n",
    "        except Exception as e:\n",
    "            print(f\"Error in get_diverse_vector_results: {e}\")\n",
    "            return []\n",
    "\n",
    "    def format_vector_results(self, results: list, max_text_length: int = 1200) -> str:\n",
    "        \"\"\"\n",
    "        Format vector results.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        results : list\n",
    "            A list of vector results.\n",
    "        max_text_length : int\n",
    "            The maximum text length.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        str\n",
    "            The formatted vector results.\n",
    "        \"\"\"\n",
    "        context = \"\"\n",
    "        for i, result in enumerate(results, 1):\n",
    "            context += f\"Document {i}:\\n\"\n",
    "            context += f\"Text: {result['text'][:max_text_length]}...\\n\"\n",
    "            if 'metadata' in result and 'topic' in result['metadata']:\n",
    "                context += f\"Topic: {result['metadata']['topic']}\\n\"\n",
    "            context += f\"Relevance Score: {result['score']:.4f}\\n\\n\"\n",
    "        return context\n",
    "\n",
    "    def format_context(self, path_info: list, max_text_length: int = 400) -> str:\n",
    "        \"\"\"\n",
    "        Format context.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        path_info : list\n",
    "            A list of path information.\n",
    "        max_text_length : int\n",
    "            The maximum text length.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        str\n",
    "            The formatted context.\n",
    "        \"\"\"\n",
    "        context = \"Information extracted from the document graph:\\n\\n\"\n",
    "        for i, path in enumerate(path_info, 1):\n",
    "            context += f\"Path {i}:\\n\"\n",
    "            for node_info in path:\n",
    "                context += f\"- Topic: {node_info['topic']}\\n  Text: {node_info['text'][:max_text_length]}...\\n\"\n",
    "            context += \"\\n\"\n",
    "        return context\n",
    "\n",
    "    def initialize_graph(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the graph.\n",
    "        \"\"\"\n",
    "        self.nx_graph = nx.Graph(self.embeddings.graph.backend)\n",
    "        self.id_mapping = self._create_id_mapping()\n",
    "\n",
    "    def _create_id_mapping(self) -> dict:\n",
    "        \"\"\"\n",
    "        Create an ID mapping.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary mapping node IDs to node objects.\n",
    "        \"\"\"\n",
    "        return {self.embeddings.graph.attribute(node, \"id\"): node for node in self.nx_graph.nodes()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "991b15e8-d001-4317-ab0f-19a2b69b6f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "analyzer = KnowledgeGraphAnalyzer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
